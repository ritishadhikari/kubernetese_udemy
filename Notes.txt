Docker Container shares the underlying kernel. 

Each Docker Containers has the additional softwares that makes the Operating Systems different and Docker utilizes the underlying 
kernel of docker hosts which works with all the operating systems above.

We won't be able to run a windows based container on a docker host as linux os on it. For that we would require a Docker on a Windows Server. 

The main purpose of Docker is to containerize applications, to ship them and run them.

Incase of docker we have the underlying hardware infrastructure, then the OS, and then Docker installed on the OS. Docker can then manage the 
containers that run libraries and dependencies alone. 

Incase of Virtual Machines we will have the OS on the underlying hardware and then the hypervisor like ESX and then the Virtual Machines. 
Each Virtual Machines will have independent Operating Systems inside of it. And then the dependencies and then the applications. 
This overheads causes higher utilization of underlying resources as there are multiple virtual operating systems and kernels running. 
The Virtual Machine consumes higher disk space as each VM takes up GBs. 

Docker Container can boot up faster, where as Virtual Machines boots up considerably slower as it boots up the entire operating system. 

VM has complete isolation from each other but docker do not have complete isolation as resources are shared. 

An Image is a template. It is used to create one or more containers. 

Container are running instances of images that are isolated and have their own environments and sets of processes. 

With Docker a major portion of works involved in setting up the infrastructure is now in the hands of the developers in the form of a Dockerfile. 

The whole process of automatically deploying and managing containers is known as containers orchestration. 

Kubernetes is a container orchestration technology. It is a bit difficult to setup and get started but provides a lot of options to customize 
deployments and supports complex architectures. 

Through Container orchestration, our application is now highly available as hardware failures do not bring our applications down, 
because we have multiple instances of our applications running on different nodes. The user traffic is load balanced across the various containers. 

When demand increases, deploy more instances of the applications seemlessly and within a matter of seconds and we have the ability to do that 
at a service level.

When we run out of hardware resources, scale the number of underlying nodes up or down without having to take down the application. 
And do all these with declarative object configuration files. 

A node is a machine, physical or virtual where kubernetes is installed. A node is a worker machine, and that is where a container will be 
launched by kubernetes.

A cluster is a set of nodes grouped togather. In this way, even if one node fails, we still have our cluster accessible from the other nodes. 

Having multiple nodes helps in sharing as well. 

The master is another node where kubernetes is installed in it and is configured as a master. The master watches over the nodes in the cluster, and is responsible
for the actual orchestration of the containers on the worker nodes. 

When we are installing kubernetes on a system, we are actually installing the following components: 
-API server: Acts as the frontend for kubernetes. The users, management devices, command line interfaces all talk to the API server to interact with the 
kubernetes cluster.
-etcd service: Distributed, reliable key-value store to store data to manage the cluster. It is responsible for implementing locks within the cluster,
to ensure there are no conflicts within the masters.
-kubelet service: It is the agent that runs on each nodes in the cluster. It is responsible for making sure that the containers are running on the nodes
as expected. 
-container runtime: It is the underlying software that is used to run containers. Happens to be docker. 
-controller: Brain behind the orchestration. They are responsible for noticing and responding when nodes, containers or endpoint goes down. It makes decisions
to bring up new containers in such cases. 
-scheduler: It is responsible for distributing work or containers across multiple nodes. It looks for newly created containers and assigns them to nodes. 

The workers nodes is where the containers are hosted. The master server has the cube API server and that is what makes it a master. The worker nodes
have the kubelet agent which is responsible for interacting with the master to provide the health information of the worker nodes and carry out actions requested
by the master on the worker nodes. 

All the information gathered are stored in the key-value pair stored in the master which is based on the etcd framework. 

The master also has the controller and the scheduler.

kubectl is a command line tool also called kube-control. This tool is used to manage and deploy application on a kubernetes cluster. It is used to get the 
cluster information, to get the status of other nodes in the cluster and to manage many other things. 

The kubectl run hello-minikube command is used to deploy an application on the cluster. 

The kubectl cluster-info command is used to view information about the cluster. 

The kubectl get nodes command is used to list all the nodes part of the cluster.

Minikube is the easiest way to get started with Kubernetes on a local system. 

It would take a lot of time to setup and install the master and the worker nodes with the above components on different systems individually by ourselves. 

Minukube bundles all of this different components into a single image providing us a preconfigured single node kubernetes cluster which we can get started
in a matter of minutes. It is packaged into an ISO image and is available online for download. 

Minikube provides an executable command line utility that will automatically download the ISO and deploy it on virtualization platforms such as Virtual Box or
VMWare fusion. Hence we must have an Hypervisor installed on our system. To interact with the kubernetes cluster, we must have the kubectl kubernetes tool also 
installed on our machine. 

kubeadm is used for setting up a multi-node Kubernetes cluster in a local environment. 

A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates.

kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4: Deploy a Kubernetes Container called hello-node by downloading the image
k8s.gcr.io/echoserver:1.4

kubectl get deployments: View the Deployments made till now. 

kubectl expose deployment hello-node --type=LoadBalancer --port=8080: Expose the deployment that was created earlier through a particular port. 

minikube service hello-node --url: Get the url where the service is running.  This will be a proof, that our setup is working. 

delete service hello-node: Delete the service. Not the deployment

delete deployment hello-node: Delete the Deployment. 

kubectl get nodes -o wide: Check the Version of Kubernetes installed. 

With Kubernetes our ultimate aim is to deploy our applications on a set of machines that are configured as worker nodes in a cluster. 

However Kubernetes does not deploy containers directly on worker nodes. The containers are encapsulated into a Kubernetes Object known as pods. 

A pod is a single instance of an application. A Pod is the smallest object that can be created in Kubernetes. 

When there are more loads we do not spin up new container instances on the same pod, but we create new pod altogather with a new instance of the 
same application. 

When userbase further increases, and the current node does not have sufficient capacity, then we can deploy additional pods in a new node 
in the cluster. 

We will have a new node in the cluster to expand the clusters physical capacity. 

Pods usually have a one-one relationship with containers running your application. To scale up we create new pods, 
and to scale down, we delete existing pods. 

A single pod can have multiple containers, except for the fact that they are usually not multiple containers of the same kind. 

If our intention was to scale our application, then we would need to create additional pods.

Incase we want a helper containers to live alongside our application container, then we can have both of these containers as part of the same pod. 
Both are created togather and both are deleted togather. 

The two containers in a same pod can also communicate with each other directly, by referring to each other as local host, since they share the 
same network space. Plus they can share the same storage space as well. 

We just need to define Kubernetes what containers a pod consists of and the container in a pod by default will have access to the same storage, 
the same network namespace and they will be created and destroyed togather. 

kubectl run nginx --image nginx: Deploys a Docker Container by creating a pod. It first creates a pod automatically and deploys an instance of 
nginx docker image. 

kubectl get pods: Helps us see the list of pods in our cluster. We also see the status of the pods - from container creating state to a running state when it is
actually running. 

kubectl describe pod nginx: Provides a lot more information compared to the get pods command. 

kubectl get pods -o wide: Gives the information about the nodes where the pod is running. 

Each Pod gets an internal IP of its own within the Kubernetes cluster. 

Json:
Fruits:[Banana:{Calories: 105,Fat: 0.4 g,Carbs: 27 g},
		 Grape:{Calories: 62,Fat: 0.3 g, Carbs: 16 g}]
		 
Yaml:
Fruits:
	- Banana:
		Calories: 105
		Fat: 0.4 g
		Carbs: 27 g
	- Grape: 
		Calories: 62
		Fat: 0.3 g
		Carbs: 16 g


Employee:
  Name: Jacob
  Sex: Male
  Age: 30
  Title: Systems Engineer
  Projects:
    - Automation
    - Support
  Payslips:
    - Month: June
      Wage: 4000
	- Month: July
      Wage: 4500
	- Month: August
      Wage: 4000

Kubernetes uses yaml files as inputs for the creation of objects such as pods, replicas, deployments.

A Kubernetes definition file always contains 4 top level fields:
- apiVersion: Version of the kubernetes API we are using to create the objects.
The maping for kind/apiVersion are: (POD,v1), (Service,v1), (ReplicaSet,apps/v1),(Deployments,apps/v1)
- kind: The kind refers to the type of object we are trying to create which in this case happens to be a Pod/Service/ReplicaSet/Deployment.
- metadata: The metadata is data about the object like its name, labels etc. This is in the form of a dictionary. 
The name under metadata is a string value which is used to name your pod.
The Label is a dictionary 
- spec: This is where we would provide additional information to kubernetes pertaining to that object. This is going to be different for different objects.
Spec is a dictionary. So we can add a property under it called containers which is list because a pod can have multiple containers within them. 

once the yaml file is created, we can run 'kubectl create -f pod-defintion.yml'

After this run 'kubectl get-pods' to see the list of pods available. 

If for some reasons our application crashes and the pod fails? Then users will no longer be able to access our application. To prevent users from
loosing our application, we would like to have more than one instance or pod running at the same time. That way if one fails, we would still 
have our application running on the other one.

The Replication controller, helps run multiple instances of a single pod in the Kubernetes Cluster, thus providing high availability. 

Even you have a single pod, the replication controller can help by automatically bringing up a new pod when the existing one fails. 

Thus the replication controller ensures that the specified number of pods are running at all times, even if it is just one or hundred. 

Another reason for having replication controller is to create multiple pods to share the loads across them. If we have a single pod serving 
a set of users, when the number of users increases, we deploy additional pods to balance the loads across the two pods. 

We could deploy additional pods across the other nodes in the cluster. In this case, the replication controller helps balance the load
across multiple pods on different nodes as well as scale our application when the demand increases. 

ReplicaSet and Replication controller have similar purpose, but they are not the same. Replication Controller is an older technology
which is substituted by the replica set. 

ReplicaSet requires a selector definition. The Selector section helps the replica set what pods falls under it. ReplicaSet can also manage
Pods that were not created as part of the replica set creation. 

kubectl create -f .\rc-definition.yml command is used to run the replica controller pods. 

kubectl get replicationcontroller: Command to get the status of the replication controller. 

kubectl create -f .\replicaset-definition.yml command is used to run the replica set pods.

A Replica Set is a process that monitors the pods. How does the replicaset know what pods to monitor. There could be hundreds of other pods in
the cluster running different applications. This is where labelling of pods during creation comes in handy. We can now provide this labels as
a filter for replicaset. This way the replica set knows which Pods to monitor. 

Incase we want to increase our replica set then we can edit our yml file and change the number of replicas. And then use the below command: 
kubectl replace -f .\replicaset-definition.yml. 

Or we can write the below command without editing the yml file internally:
kubectl scale --replicas 6 -f .\repliaset-definition.yml

We can also write: kubectl edit replicaset myapp-replicaset to edit the number of pods and the new number of pods will run accordingly. 

kubectl delete replicaset myapp-replicaset: Delete a Replicaset which was defined under metadata. 

Deployment is a Kubernetes object that comes higher in the hierarchy. The deployment provides us with the capability
to upgrade the underlying instances seamlessly using rolling updates, undue changes, resume
changes as required. 

kubectl create -f .\deployment-definition.yml to deploy the kubernetes yml file. 

kubectl get deployments: to get the deployment status after deploying the yml file.   

we can also use kubectl scale deployment --replicas=3 for scaling with the replica set.

When we first create a deployment, it triggers a rollout. A new rollout creates a new deployment 
revision.  This helps us keep track of the changes in our deployment and enables us to 
rollback to previous versions of deployment if necessary.

kubectl rollout status deployment/myapp-deployment: Gives the status of our rollout.

kubectl rollout history deployment/myapp-deployment: Shows the history of our deployment. 

There are two types of deployment strategies: 
i. Recreate Strategy: Destroy the n running old instances and then create the n new instances. 
The time period between when the older versions are down and newer versions are up, the application
is down and is inacsessible to the users. This is not the default strategy. 
ii. Rolling Update: We do not destroy all the n old instances at once. We take down the older version 
and bring up the newer version one by one. This way the application never goes down and the 
application is seamless. 

We update the strategy through: kubectl apply -f deployment-definition.yml
With this a new rollout is triggered and a new revision of the deployment is created. 

We can also do the revision through: kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
This way the deploymment definition file will have a different configuration. 

Upgrade: When a new deployment is created, it creates a replica set automatically, which in turn creates
the number of pods which in turn meets the number of replicas. 

When we upgrade the application, the kubernetes deployment object creates a new replica set under 
the hood, and starts deploying the container there, and at the same time taking down the pod in the
old replica set following a rolling update strategy. 

Kubernetes deployment allow you to rollback to a previous revision. 

kubectl rollout undo deployment/myapp-deployment: To undo a change, by bringing the older pods up and running
while destroying the newer pods. 

Check which replica is down and which replica is up through: kubectl get replicasets

kubectl create -f .\deployment.yml --record: We can record our deployments throught his command.

kubectl set image deployment myapp-deployment nginx=nginx:1.18-perl --record: To update the deployment
and also record through another way. 

In the Kubernetes world, the IP is assigned to a Pod and not a container.

Each Pod in Kubernetes gets its own internal IP Address. 

The pods communicate with each other through the parent IP. 

The internal IP address, which connects multiple pods in a specific node, for two different node is same.

But if two pods in two different nodes has the same IP address, then it leads to IP conflicts. 

Kubernetes does not automatically setup any kind of networking to handle these issues. 

Kuberenetes expects manual intervention to setup certain fundamental requirements.

NAT (Network Address Translation) translates one set of IP adresses into another set of IP addresses. 
It translates from Private to Public and also from Public to Private IP addresses. 

All containers/PODs can communicate to one another without NAT.

All nodes can communicate with all containers and vice-versa without NAT.

Some pre built networks helps for this cause. Ex. Cilium, Flannel, etc. 

Hence with all the above mentioned cluster networking setup, it now manages the networks and IPs in the
nodes and assigns it different network address for each network in the node.

This creates a virtual network of all pods and nodes where they are all assigned a unique IP address, and by using simple routing 
techniques, the cluster networking enables communication between the different pods or nodes to meet the networking requirements of kubernetes.

Kubernetes Services enables communication between within and outside of the application. 

Kubernetes services helps connect applications togather with other applications or users. 

It is services that enables connectivity between groups of pods. 

Services enables loose coupling between microservices in our application. 

The Kubernetes Service is an object just like pods, replica sets or deployments, one of its use case is 
to listen to a port on the node and forward request on that port to a pod running the web application. 

This type of service is known as node port service, because the service listens to a port 
on the node and forward requests to the pod.

Types of Services: 
1. NodePort: Service makes an internal pod accessible on a port on the node. 
2. ClusterIP: The service creates a virtual IP inside the cluster to enable communication between different
services, such as a set of front end servers to a set of back end servers. 
3. Load Balancer: Provisions a Load Balancer for our application in supported cloud providers. 

NodePort Kuberenetes Service: 
Three Ports involves. 
a. Port on the Pod: Port where the actual web server is running. Referred to as the target port.  
b. Port on the Service: Referred to as the Port. Service is like a virtual server inside the node. 
c. Port on the Node: We access the webserver externally. Known as the Node Port. Node Port can be 
within a valid range of 30000 to 32767 only. 


We use a definition file to create a service. 

kubectl create -f service-definition.yml:  Execute the yml file to enable the service.

kubectl get services: Get the list of running services. 

In a production environment, we have multiple instances of our web application running for high availability
and load balancing purposes. In this case we have multiple singular pods, running our web application. 

All the pods have the same labels, the key app and set to a value myapp.

The same label is used as a selector during the creation of the service. 

When the service is created, it looks for a matching pod with the label and finds n of them. 

The Service then automatically selects n pods as endpoints to forward the requests coming from the 
external users. 

When the pods are distributed across multiple nodes, Kubernetes automatically creates a service that 
spans across all the nodes in the cluster and maps the target port to the same node port on all the
nodes in the cluster. 
 
This way you can access your application using the IP of any node in the cluster
and using the same port number of the node.  
 
minikube service myapp-service --url: Return the URL for the service .
 
A kubernetes service can help group the pods togather and provide a single interface to access the pods in a
group.  